# 4.3.6 Migration from Docker swarm guide

## Migration Prerequisites and Important Information

This tutorial guides you through all steps required to transform your Docker Swarm infrastructure to Kubernetes and redeploy OpenCRVS on it. **Please carefully review this section before proceeding with the migration.**

### Prerequisites

#### 1. Infrastructure Repository

OpenCRVS v2.0+ uses a dedicated repository for continuous delivery configuration:

**Repository:** [https://github.com/opencrvs/infrastructure](https://github.com/opencrvs/infrastructure)

**Required Action:**

* Fork this repository to your organization or personal GitHub account before starting migration
* Detailed forking steps are provided in [#id-4.-fork-or-clone-repositories](4.3.1-preparation-steps/4.3.1.4-create-prerequisite-accounts-and-repositories.md#id-4.-fork-or-clone-repositories "mention")

#### 2. SSH Access Configuration

The `provision` user on the Docker manager server must have passwordless SSH access to the backup server. You will need configure SSH key-based authentication between Docker manager and backup server.

**Steps:**

1.  Login on production docker manager and get `provision` user public key:<br>

    ```
    sudo -i
    su - provision
    ssh-keygen -N "" -t ed25519 -f /home/provision/.ssh/id_ed25519 > /dev/null && \
    echo '' && \
    cat /home/provision/.ssh/id_ed25519.pub
    ```
2.  Copy public key value from output (example value):

    ```
    provision@prod-1:~$ ssh-keygen -N "" -t ed25519 -f /home/provision/.ssh/id_ed25519 && echo '' && cat /home/provision/.ssh/id_ed25519.pub

    ssh-ed25519 AAAAC3NzaC1lZDI1NTE5pxmslgDlhTRXKbc7mUReqhDUla+8nm8JJc6UFvRb47r provision@prod-1
    ```
3.  Login on backup server as provision user and add public key from production to `/home/provision/.ssh/authorized_keys` on backup server:<br>

    ```
    sudo -i
    su - provision
    echo 'ssh-ed25519 AAAAC3NzaC1lZDI1NTE5pxmslgDlhTRXKbc7mUReqhDUla+8nm8JJc6UFvRb47r provision@prod-1' > /home/provision/.ssh/authorized_keys
    ```

### Supported Environments

#### ✅ Environments Supported by Migration Workflow

The migration workflow supports **only** the following environments:

* `development`
* `qa`
* `staging`
* `production`

Following environments are deprecated:

* `backup` is merged into production while migration and is managed as part of production environment by Provision workflow.
* `jumpbox` is not used anymore

### Docker-compose migration

All OpenCRVS services are deployed as helm charts during migration.

All customizations must be **re-implemented using custom Helm chart**.

Example custom helm chart is published as part of infrastructure repository, see [https://github.com/opencrvs/infrastructure/tree/develop/charts/opencrvs-mosip](https://github.com/opencrvs/infrastructure/tree/develop/charts/opencrvs-mosip)

You may also choose to use [Bitnami Common Library Chart](https://github.com/bitnami/charts/tree/main/bitnami/common) for more advanced use cases.

### Migration Architecture

#### Single Server Deployments

* **Docker Swarm:** Single server environment
* **Kubernetes:** Converted to single server Kubernetes environment (single-node cluster)

#### Multi-Node Deployments

* **Docker Swarm:** Multi-node with manager and worker nodes
* **Kubernetes:** Converted to Kubernetes cluster with worker nodes

**⚠️ Important Limitation:**

* Migration script **does not support** multiple Docker manager nodes
* Only single manager + multiple workers configuration is supported

Single server docker environments are converted to single server Kubernetes environments.

Multi node docker environments are converted to Kubernetes clusters with worker nodes. Migration script doesn't support multiple docker manager nodes.

## Migration process

1. Navigate to Country config repository
2.  Run "Migration swarm to k8s" GitHub actions workflow

    <figure><img src="../../../.gitbook/assets/image (22).png" alt=""><figcaption></figcaption></figure>

    1. "The environment to migrate": Environment choosen for migration, you may migrate environments one by one or altogether
    2. "The target organisation owner": GitHub organisation or your personal account who owns Infrastructure repository
    3. "The target infrastructure repository": GitHub infrastructure repository name
3.  Verify workflow execution results:\
    On the screenshot execution results for single environment are shown, make sure all steps completed successfully.

    <figure><img src="../../../.gitbook/assets/image (20).png" alt=""><figcaption></figcaption></figure>
4. Verify target repository:
   1. Navigate to Infrastructure repository
   2. Navigate to Settings -> Environments: Make all repository and environment level secrets and variables have been created
   3.  Navigate to Settings -> Actions -> Runners: Make sure self-hosted runners are available for each migrated environment. If you run migration for one environment only, you will find only one runner. NOTE: Environment name is a part of runner name and one of the runner tags, e/g "development" on the screenshot

       <figure><img src="../../../.gitbook/assets/image (23).png" alt=""><figcaption></figcaption></figure>
5.  Navigate to "Pull requests" section: You will find PR with all changes required to deploy new environment with Kubernetes.

    <figure><img src="../../../.gitbook/assets/image (25).png" alt=""><figcaption></figcaption></figure>
6. Review changes within PR. If your docker-compose file have any customisations like environment variables, please add them to `environments/<env name>/opencrvs-services/values.yaml`
7. Merge Pull request to main (develop) branch.
8. Provision Kubernetes environment on top of Docker Swarm environment: Run Provision workflow from Infrastructure repository, see for more information [4.3.5-provisioning-servers](4.3.5-provisioning-servers/ "mention")
9. Reboot your target server
10. Deploy dependencies, see for more information [4.3.5.1-running-a-dependencies-deployment.md](4.3.6-deploy/4.3.5.1-running-a-dependencies-deployment.md "mention")
11. Deploy OpenCRVS: see for more information [4.3.5.2-running-a-opencrvs-deployment.md](4.3.6-deploy/4.3.5.2-running-a-opencrvs-deployment.md "mention")
    1. Use same OpenCRVS Core image tag as docker swarm
    2. Use same Country config image tag as docker swarm
    3. Make sure "Enable data seeding during deployment" is unchecked

## Post migration steps

### Configuring Workflow Approval Process

OpenCRVS now requires approval for deployment and maintenance GitHub Actions workflows on production environments. This adds an additional layer of control to prevent unexpected downtime or issues.

#### Environments Requiring Approval

* **Production:** Approvals required by default
* **Staging and Other environments:** No approval required (unless explicitly configured)

#### Setup Instructions

**Step 1: Identify Approvers**

Determine which team members should have authority to approve production deployments and maintenance tasks.

**Step 2: Configure Repository Variable**

Set up the `GH_APPROVERS` variable at the **repository level**:

1. Navigate to your repository settings
2. Go to **Settings → Secrets and variables → Actions → Variables**
3. Click **New repository variable**
4. Configure:
   * **Name:** `GH_APPROVERS`
   * **Value:** Comma-separated list of GitHub usernames (e.g., `alice,bob,charlie`)

Set Up `APPROVAL_REQUIRED` at e**nvironment level** to control which environments require approval before workflow execution:

1. Navigate to your repository settings
2. Go to **Settings → Environments**
3. Select the environment you want to configure (e.g., `production`, `staging`)
4. Scroll to **Environment variables** section
5. Click **Add variable**
6. Configure:
   * **Name:** `APPROVAL_REQUIRED`
   * **Value:** `true` (to enable approval) or `false` (to disable)

**Step 3: Verify Configuration**

After configuration, workflows targeting production will pause for approval from designated users before execution.

**Important Notes**

* `GH_APPROVERS` must be defined at **repository level** (not environment level)
* `APPROVAL_REQUIRED` must be defined at **environment level** (not repository level)
* Use **GitHub usernames** without `@` symbol
* Multiple approvers separated by commas with **no spaces**
* At least one designated approver must approve the workflow to proceed

### Backup & Restore

{% hint style="info" %}
Please read this section to get better understanding about changes, no actions is needed from your side
{% endhint %}

This section outlines the fundamental differences in backup and restore implementation between Docker Swarm and Kubernetes deployments. Understanding these differences is critical for migration planning and operational adjustments.

#### Docker Swarm Approach

**Architecture:**

* Backup and restore operations run as **shell scripts on the Docker manager node**
* Backup script scheduled as **OS cronjob** on production environment
* Restore script scheduled as **OS cronjob** on staging environment
* Single monolithic script handles **all datastores** (PostgreSQL, MongoDB, MinIO, InfluxDB)
* Uses **direct filesystem access** to `/data` directory

#### Kubernetes Approach

**Architecture:**

* Backup and restore jobs **split by datastore** (one job per database)
* Backup and restore operations are environment-agnostic and can be configured on any deployment (development, QA, staging, production, etc.).
* Scheduled as **Kubernetes CronJobs** (native k8s resources)
* Scripts use **remote database connections** via network protocols
* Each job runs as a **Kubernetes pod** with database client tools

The fundamental shift is from **filesystem-based operations** to **network-based database operations**. While this introduces network overhead, it provides the flexibility and portability required for modern cloud-native deployments, including support for managed database services and multi-environment consistency.

Check [4.3.7-backup-and-restore](../4.4-opencrvs-maintenance-tasks/4.3.7-backup-and-restore/ "mention") for more information and configuration details.

