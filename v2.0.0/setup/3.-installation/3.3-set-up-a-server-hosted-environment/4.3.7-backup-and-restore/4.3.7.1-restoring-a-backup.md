# 4.3.7.3 Automated restore configuration

{% hint style="danger" %}
Before going live, it's very important that you confirm that OpenCRVS is successfully backing up and restoring. Make a test registration in **production** with image attachments, and **wait 24 hours**. This test registration should exist in your **staging** environment the day after you created it. "View" the registration on **staging** and make sure that image attachments are available. To learn more about all the technical checks that you should perform before going live with your OpenCRVS installation, read the [Pre-deployment Checklist](../../../6.-go-live/3.3.4-set-up-an-smtp-server-for-opencrvs-monitoring-alerts.md) section. To reset your all your environments including production in readiness for going live, read the [reset](../4.3.6-maintenance/4.3.6.5-resetting-a-server-environment.md) section.
{% endhint %}

### Automated restore configuration

{% hint style="info" %}
Usually restore is configured on staging server. In this way, your staging server contains citizen data that is 24 hours old. The staging server can be used as a pre-production mirror for you to test any OpenCRVS upgrades or configuration changes on real citizen data without disturbing your production environment.
{% endhint %}

Usually restore job is configured on staging environment. Backup server is part of production inventory file. Before configuring restore copy **`backup-server-ssh-credentials`** and **`restore-encryption-secret`** secrets **from production to staging, see** [**Copy secrets from production to staging**](4.3.7.1-restoring-a-backup.md#copy-secrets-from-production-to-staging)**.**

More information is in [restore guide](https://github.com/opencrvs/infrastructure/blob/develop/charts/dependencies/README.md#restore-configuration).

Add following section to `environments/<environment>/dependencies/values.yaml`  and run "Deploy dependencies" workflow:

* Update `backup_server_dir` value to match with your environment name, e/g `development`
* Update `schedule` to reflect best time backup job to be started,&#x20;
* Set `enabled` to `true`

```
# Restore configuration
restore:
  enabled: true
  # Schedule job
  schedule: "0 0 * * *"
  # Secret to store backup server connection configuration:
  # - user, backup server username
  # - host, hostname / IP of backup server
  # - ssh_key, ssh key for passwordless connection
  backup_server_secret: backup-server-ssh-credentials
  backup_server_dir: /home/backup/production
  backup_encryption_secret: restore-encryption-secret
```

Push your changes to github and Re-[Deploy Dependencies](../4.3.6-deploy/4.3.6.1-running-a-deployment.md)

### Copy secrets from production to staging

{% hint style="info" %}
Its recommended to configure connection to cluster as described at [Add new cluster to your default kubeconfig](../4.3.5-provisioning-servers/4.3.5.2-kubernetes-cluster-access.md#option-3-add-new-cluster-to-your-default-kubeconfig) before running this task.
{% endhint %}

1.  Connect to production cluster with `kubectl` :

    ```
    kubectl config use-context <production environment context>
    ```
2.  Change namespace to opencrvs-deps-\<environment>, where `<environment>` is production environment name:

    ```
    kubectl config set-context --current --namespace=opencrvs-deps-<environment>
    ```
3.  Get secrets as yaml manifest:\


    ```
    kubectl get secret backup-encryption-secret \
      backup-server-ssh-credentials -oyaml > /tmp/production-backup-secrets.yaml
    ```
4. Update file `/tmp/production-backup-secrets.yaml` :
   1. Delete following fields:
      1. creationTimestamp
      2. resourceVersion
      3. uid
      4. annotations
      5. namespace
   2. Update `name: backup-encryption-secret` to `name: restore-encryption-secret`&#x20;
5.  Connect to stagomg cluster with `kubectl` :

    ```
    kubectl config use-context <staging environment context>
    ```
6.  Change namespace to opencrvs-deps-\<environment>, where `<environment>` is stagig environment name:

    ```
    kubectl config set-context --current --namespace=opencrvs-deps-<environment>
    ```
7.  Create secrets:

    ```
    kubectl apply -f /tmp/production-backup-secrets.yaml 
    ```

    Example output:

    ```
    secret/restore-encryption-secret created
    secret/backup-server-ssh-credentials created
    ```

Example how your temporal file will look before your apply changes:

```yaml
apiVersion: v1
items:
- apiVersion: v1
  data:
    backup_encryption_key: dEV...SQ==
  kind: Secret
  metadata:
    name: restore-encryption-secret
  type: Opaque
- apiVersion: v1
  data:
    host: M...M=
    ssh_key: LS0hr...S0VZLS0tLS0K
    user: YmFja3Vw
  kind: Secret
  metadata:
    name: backup-server-ssh-credentials
  type: Opaque
kind: List
metadata:
  resourceVersion: ""
```

### Verify restore configuration

Usually to make sure restore was successful you need to verify data from Production is present on Staging environment. Additional verification steps are described here.

**Verify kubernetes jobs are present:**

1. Connect to your cluster with `kubectl`
2.  Run following command:

    ```
    kubectl get cronjob -l job-type=restore -n opencrvs-deps-<environment>
    ```

    Example output:

    ```
    NAME               SCHEDULE    TIMEZONE   SUSPEND   ACTIVE   LAST SCHEDULE   AGE
    influxdb-restore   0 0 * * *   <none>     False     0        <none>          62s
    minio-restore      0 0 * * *   <none>     False     0        <none>          62s
    mongodb-restore    0 0 * * *   <none>     False     0        <none>          62s
    postgres-restore   0 0 * * *   <none>     False     0        <none>          61s
    ```

**Verify Kubernetes jobs were executed per schedule:**

Wait at least for first job execution, usually takes at to 24 hours

1. Connect to your cluster with `kubectl`
2.  Run following command:

    ```
    kubectl get job -l job-type=restore -n opencrvs-deps-<environment>
    ```

    Example output:

    ```
    TODO: FIXME:
    NAME                       STATUS     COMPLETIONS   DURATION   AGE
    influxdb-backup-29381820   Complete   1/1           9s         5h11m
    minio-backup-29381820      Complete   1/1           9s         5h11m
    mongodb-backup-29381820    Complete   1/1           31s        5h11m
    postgres-backup-29381820   Complete   1/1           13s        5h11m
    ```

