---
description: >-
  Critical information required to understand how to regularly backup and
  restore your citizen registration data in case of a server problem.
---

# 3.3.8 Automated & manual backup and manual restore

### Automated backup process to an external server

If you configured a separate backup server and used the external backup server optional SSH properties in the Ansible script in step [3.3.2 Install dependencies](3.3.2-install-dependencies.md), then every day OpenCRVS automatically backs up all databases to the following directories on the manager node.

Every 7 days the Mongo data is deleted to save disk space. The Ansible playbook sets this up as an option in this [step](3.3.2-install-dependencies.md). The files are backed up to that server in the middle of the night.

{% hint style="warning" %}
TO AVOID CITIZEN DATA LOSS... You must configure and test this automated backup process in production. Operationallly, we highly recommend that once a week, these files should be saved to a password protected and encrypted external harddrive and stored in a secure and approved location.
{% endhint %}

### Backup files

Mongo backups are saved as mongo .gz zip files using the date of the backup here:

```
/data/backups/mongo/hearth-dev-<date>.gz
/data/backups/mongo/openhim-dev-<date>.gz
/data/backups/mongo/user-mgnt-<date>.gz
/data/backups/mongo/application-config-<date>.gz
/data/backups/mongo/metrics-<date>.gz 
/data/backups/mongo/webhooks-<date>.gz // Only exists if a webhook integration is configured
```

Elasticsearch snapshot files and indices are saved here.

{% hint style="warning" %}
**The entire elasticsearch folder contains all snapshots and must be preserved indefinitely**
{% endhint %}

```
/data/backups/elasticsearch
```

InfluxDB backup files are saved into a date named directory here:

```
/data/backups/influxdb/<date>
```

_**Commencing with OpenCRVS v1.2,**_ Minio attachments are saved as a .gz zip file using the date of the backup here:

```
/data/backups/minio/ocrvs-<date>.tar.gz
```

### Manual backup process

You can manually run the automated backup script at any point.

1. SSH into your server and navigate to the following directory:

```
cd /opt/opencrvs/infrastructure/
```

2\. Ensure that your database [secrets](3.3.6-deploy-automated-and-manual.md) are available to the script as environment variables. You can do this by running:

```
export ELASTICSEARCH_ADMIN_USER=elastic \
export ELASTICSEARCH_ADMIN_PASSWORD=<your elastic password> \
export MONGODB_ADMIN_USER=< your mongo username> \
export MONGODB_ADMIN_PASSWORD=<your mongo password>
```

3\. The backup script is run like this, with the parameters explained below. Even if you have not set up an external server that the manager node can SSH into, the files will still backup to the directory locations above. Just the SSH part will fail. So once the script finishes you can use rsync to copy the files manually off your server and into a local directory.

{% hint style="info" %}
If you are migrating between versions of OpenCRVS, you could use the SSH details for your new server running a newer version of OpenCRVS.
{% endhint %}

Replace and separate the \<parameters> with a space when calling the script

```
bash ./emergency-backup-metadata.sh --ssh_user=<SSH_USER> --ssh_host=<SSH_HOST> --ssh_port=<SSH_PORT> --production_ip=<PRODUCTION_IP> --remote_dir=<REMOTE_DIR> --replicas=<REPLICAS> --label=<LABEL>
```

<table><thead><tr><th width="189.33333333333331">Parameter</th><th width="139">Optional or Mandatory</th><th>Descrption</th></tr></thead><tbody><tr><td>SSH_USER</td><td>Mandatory</td><td>This is the SSH user for your backup server. If running manually you can enter anything, such as: <strong>root</strong></td></tr><tr><td>SSH_HOST</td><td>Mandatory</td><td>This is the IP address for your backup server. If running manually you can enter anything, such as: 132.42.41.15</td></tr><tr><td>SSH_PORT</td><td>Mandatory</td><td>This is the SSH port for your backup server. If running manually you can enter anything, such as: <strong>22</strong></td></tr><tr><td>PRODUCTION_IP</td><td>Mandatory</td><td>This is the public IP address for the manager server node for your OpenCRVS cluster. It must be entered correctly.</td></tr><tr><td>REMOTE_DIR</td><td>Mandatory</td><td>This is the directory path on the backup server that you wish to copy the backup files into. E.G. <strong>/root/opencrvs-backups</strong>. If running manually you can enter anything.</td></tr><tr><td>REPLICAS</td><td>Mandatory</td><td>The number of servers in your OpenCRVS cluster. This value can only be equal to: <strong>1, 3</strong> or <strong>5</strong></td></tr><tr><td>LABEL</td><td>Optional</td><td>Normally, today's date is used as a suffix to name the backup files in the directories as explained above. If you want to name the backup files something different, you can enter a <strong>lowercase</strong> string here that will be used as a replacement file name suffix.</td></tr></tbody></table>



{% hint style="warning" %}
If using a custom LABEL above, it must be **lowercase**, otherwise Elasticsearch cannot make a snapshot.
{% endhint %}

4\. When complete, exit your server:

```
exit
```

5\. This command copies files from a server to your current local directory using rsync:

```


rsync -av -r --ignore-existing  --progress <ssh-user>@<opencrvs-manager-server-ip>:/data/backups .
```

###

### Manual restore process

Before you attempt a data restore, please ensure that you have read and understand these 2 warnings ...

**Warning 1: Ensure you are restoring to a new, CLEAN, OpenCRVS installation that has NOT BEEN SEEDED.**

{% hint style="danger" %}
**You should only restore to a clean installation of OpenCRVS: TO AVOID CITIZEN DATA LOSS and avoid potential issues that may be very difficult to debug. THESE COMMANDS ENTIRELY REPLACE ALL DATA AND BACKUPS ON YOUR OPENCRVS SERVER! THEY CANNOT BE REVERTED!**
{% endhint %}

**Warning 2: Prepare operationally for a restore. Test a restore on a test server first. Schedule the production restore when staff can cease operations.**

{% hint style="danger" %}
If you are restoring a backup from a previous version of OpenCRVS, we have some large data migrations to run. These migrations may take several hours to complete. In this case, we recommend performing a data restore on an entirely new set of servers. When the restore and subsequent migrations are complete, you can then change your DNS settings to point to your new server with confidence. **CEASE CIVIL REGISTRATION ACTIVITIES DURING A RESTORE. CONSIDER PERFORMING A RESTORE DURING NATIONAL HOLIDAYS TO AVOID RISK OF DATA LOSS.**
{% endhint %}

1. To perform a restore, ensure that you have backup files in the day's folders you wish to restore from. If this is a new environment, you would need to copy the backed up files and folders into the locations using scp or rsync.

Copy an Elasticsearch backup onto the server like this:

```
rsync -av --delete --progress <local-path-to-dir>/backups/elasticsearch <your-ssh-user>@<opencrvs-manager-server-ip>:/data/backups
```

Copy a Mongo backup onto the server like this:

```
rsync -av --delete --progress <local-path-to-dir>/backups/mongo <your-ssh-user>@<opencrvs-manager-server-ip>:/data/backups
```

Copy an InfluxDB backup onto the server like this:

```
rsync -av --delete --progress <local-path-to-dir>/backups/influxdb <your-ssh-user>@<opencrvs-manager-server-ip>:/data/backups
```

Copy a Minio backup onto the server like this:

```
rsync -av --delete --progress <local-path-to-dir>/backups/minio <your-ssh-user>@<opencrvs-manager-server-ip>:/data/backups
```

Copy a VS Export backup onto the server like this:

```
rsync -av --delete --progress <local-path-to-dir>/backups/vsexport <your-ssh-user>@<opencrvs-manager-server-ip>:/data/vsexport
```

**As of OpenCRVS v1.3.0**, copy a VS Export backup onto the server like this:

```
rsync -av --delete --progress <local-path-to-dir>/backups/metabase <your-ssh-user>@<opencrvs-manager-server-ip>:/data/metabase
```

2\. SSH into your server and navigate to the following directory:

```
cd /opt/opencrvs/infrastructure/
```

3. Restart Elasticsearch to pick up the new snapshots availables to it

```
docker service update --force --update-parallelism 1 --update-delay 30s opencrvs_elasticsearch
```

4\. The script is run like this, with the parameters explained below. Replace and separate the \<parameters> with a space when calling the script

```
./emergency-restore-metadata.sh --label=<LABEL> --replicas=<REPLICAS>
```

<table><thead><tr><th width="131">Parameter</th><th width="126.33333333333331">Optional or Mandatory</th><th>Description</th></tr></thead><tbody><tr><td>LABEL</td><td>Mandatory</td><td>Normally, a date is used as a suffix to name the backup files. E.G. <strong>01-01-2023</strong> but if you named the suffix in a custom way using the LABEL parameter when creating the backup, enter the <strong>lowercase</strong> file suffix for your backups here.</td></tr><tr><td>REPLICAS</td><td>Mandatory</td><td>The number of servers in your OpenCRVS cluster. This value can only be equal to: <strong>1, 3</strong> or <strong>5</strong></td></tr></tbody></table>

{% hint style="danger" %}
**The script will prompt you to answer "Yes" or "No" to continue as this is a destructive and irreversible action.**
{% endhint %}

5. You next need to take down and re-deploy OpenCRVS. This is because, every time an OpenCRVS deployment is made, database secrets that microservices use are rotated. But as you have just restored and migrated from a backup, the database secrets are now out of date. You must take OpenCRVS down. SSH into the server and run:

```
docker stack down opencrvs
```

6\. Exit the server:

```
exit
```

7\. Now, re-deploy following the [instructions](3.3.6-deploy-automated-and-manual.md) with the _--clear\_data_ property set to "**no**" of course, so as not to clear the databases.

8\. Once data has been restored to your server, migrations will optionally run on your data depending on how old the backup is. You can check the progress status of your migrations in Kibana.

{% hint style="info" %}
If you are restoring from a backup that was created using the **currently deployed version** of OpenCRVS, migrations **will not** need to run. You can skip this step and proceed to step 10 Otherwise, if you are restoring from a backup that was created using a **previous version** of OpenCRVS, migrations **will** need to run. In that case read the next warning and pay close attention to step 10.
{% endhint %}

{% hint style="warning" %}
We have an example production server, in which we have created 50,000 test birth and death registrations. Migrating the data on that server between a backup made using v1.1.\* and v1.2.\* took 5 hours to complete. **DO NOT ATTEMPT TO USE OPENCRVS WHILE MIGRATIONS ARE RUNNING**
{% endhint %}

To login to Kibana, visit _https://kibana.\<your\_domain>_ and login using the _**username: "elastic"**_ and the _**ELASTICSEARCH\_SUPERUSER\_PASSWORD**_ you configured in [this step](3.3.6-deploy-automated-and-manual.md).

In the left navigation, select **"Observability"**, then **"Logs"**

In the search bar, enter this text: **"tag: migration"**

Click the **"Stream live"** button for live output from the migration microservice. You will see output like this, potentially for many hours!!! _NOTE: After a few minutes the "Stream live" feature stops working to save resources. To get latest results, try refreshing the page in this scenario._

<figure><img src="../../../.gitbook/assets/Screenshot 2023-01-03 at 15.36.59.png" alt=""><figcaption><p>v1.1* to v1.2* migrations running</p></figcaption></figure>

When the migrations are completed, you will see the word "Done".

9\. You can login to OpenCRVS and test that all your data has restored successfully.

If for whatever reason, migrations do not run due to an error, you will see the word "PENDING" next to each migration. Please contact us on this occurance as there may be a bug requiring a hotfix. Once a hotfix has been applied, you can always re-run migrations at any time by running this command on the server:

```
docker service update --force --update-parallelism 1 --update-delay 30s opencrvs_migration
```
